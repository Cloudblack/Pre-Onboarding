{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov': 0}\n",
    "        self.fit_checker = False\n",
    "  \n",
    "    def preprocessing(self, sequences):       \n",
    "        # list(단수,복수 문자열), 문자열 입력가능\n",
    "        \n",
    "        def prepro(sequences):\n",
    "            token=sequences.lower()                #소문자 변경\n",
    "            token=re.sub(r\"[^a-z0-9 ]\",\"\",token)   #소문자와 공백,숫자 빼고 제거\n",
    "            token=token.split(\" \")                 #빈칸 기준으로 스플릿\n",
    "            result.append(token)\n",
    "            return result\n",
    "    \n",
    "        result=[]    \n",
    "        if type(sequences) == list:  #list인지 아닌지를 구분\n",
    "            for x in sequences:\n",
    "                    prepro(x)        \n",
    "        else:                        #문자열(위와 똑같이쓰면 문자별로 나눠버린다)\n",
    "            prepro(sequences)      \n",
    "        return result\n",
    "    \n",
    "  \n",
    "    def fit(self, sequences):\n",
    "        \n",
    "        self.fit_checker = False\n",
    "        result=self.preprocessing(sequences)     \n",
    "        if type(result[0]) == list:           #nested list를 flat하게 만든다\n",
    "            te=[x for y in result for x in y]\n",
    "        else:te=result\n",
    "    \n",
    "        ext_re=list(set(te))                  # 중복 제거\n",
    "        ext_re.sort()\n",
    "        for x in ext_re:\n",
    "            if x not in self.word_dict:\n",
    "                self.word_dict[x]=len(self.word_dict)   # dict 갯수 = 새로 들어올 단어의 인덱스 (0이 이미 있기떄문에)\n",
    "            else:pass     \n",
    "          \n",
    "        self.fit_checker = True\n",
    "        \n",
    "  \n",
    "    def transform(self, sequences):\n",
    "        \n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        if self.fit_checker:\n",
    "            # output 형태를 nested list으로 맞추기위해 단수 문자열도 nested list으로 출력\n",
    "            for token in tokens:\n",
    "                token_list=[]                  \n",
    "                for word in token:\n",
    "                    if word in self.word_dict:      \n",
    "                        token_list.append(self.word_dict[word]) #있으면 변환값을 넣음\n",
    "                    else:\n",
    "                        token_list.append(self.word_dict['oov']) #없으면 oov 값 출력\n",
    "                result.append(token_list)  \n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "            \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False\n",
    "        self.fit_result=[]\n",
    "        self.tfidf_matrix=[]        \n",
    "  \n",
    "    def fit(self, sequences):\n",
    "        tokenized = self.tokenizer.fit_transform(sequences) #tokenize로 변환된 문자열\n",
    "        tokens = list(set([x for y in tokenized for x in y])) #tokenized에서 중복을 제거한 리스트\n",
    "        tokens.sort()       \n",
    "        \n",
    "        def idf(word):\n",
    "            df=0\n",
    "            for text in tokenized:\n",
    "                df += word in text                \n",
    "            return math.log(n/(df+1),math.e) #log e 총 문서량/포함한 문서량 +1(분모가 0이 아니게 하려고)\n",
    "        \n",
    "        n=len(tokenized)        \n",
    "        for token in tokens:\n",
    "            self.fit_result.append(idf(token))\n",
    "        \n",
    "        self.fit_checker = True\n",
    "      \n",
    "    \n",
    "\n",
    "    def transform(self, sequences):\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences) #tokenize로 변환된 문자열\n",
    "            tokens = list(set([x for y in tokenized for x in y])) #tokenized에서 중복을 제거한 리스트\n",
    "            tokens.sort()                  \n",
    "          \n",
    "            def tf_idf(text):\n",
    "                #tf 문자열(문서) 마다 몇개씩 들어갔는지 * 이전에 구한 idf\n",
    "                return [text.count(tokens[n])*self.fit_result[n] for n in range(len(tokens))]\n",
    "            \n",
    "            for text in tokenized:\n",
    "                self.tfidf_matrix.append(tf_idf(text))\n",
    "            \n",
    "            return self.tfidf_matrix\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "  \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit_result=[]#이미 만든 객체를 다시쓸때를 위해 초기화\n",
    "        self.tfidf_matrix=[]\n",
    "        self.fit(sequences)\n",
    "        return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #확인용 판다스\n",
    "\n",
    "test=['I go to school.', 'I LIKE pizza!']\n",
    "test2=['Im so sorry~ but i love u~ Like Pizza!']\n",
    "test3='Holly Molly 2'\n",
    "test4=['If you are not sure about your level, dont worry, you can take this online test.',\n",
    "       'There are 60 multiple-choice questions and there is no time limit.',\n",
    "       'You will be able to see the answers when you finish the test.'\n",
    "       'Improve your writing with the exercises suggested in each lesson.',\n",
    "       'You will learn how to organise and connect the text in your compositions.',\n",
    "       'Different types of texts for each level: A1, A2, B1, B1+, or B2.',]\n",
    "test5 = [\n",
    "  'eat want apple',\n",
    "  'eat want banana',\n",
    "  'long yello banana banana',\n",
    "  'im furuit like'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 8, 1], [3, 8, 2], [7, 9, 2, 2], [5, 4, 6]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_test1=Tokenizer()\n",
    "\n",
    "for_test1.fit_transform(test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>banana</th>\n",
       "      <th>eat</th>\n",
       "      <th>furuit</th>\n",
       "      <th>im</th>\n",
       "      <th>like</th>\n",
       "      <th>long</th>\n",
       "      <th>want</th>\n",
       "      <th>yello</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      apple    banana       eat    furuit        im      like      long  \\\n",
       "0  0.693147  0.000000  0.287682  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.287682  0.287682  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.575364  0.000000  0.000000  0.000000  0.000000  0.693147   \n",
       "3  0.000000  0.000000  0.000000  0.693147  0.693147  0.693147  0.000000   \n",
       "\n",
       "       want     yello  \n",
       "0  0.287682  0.000000  \n",
       "1  0.287682  0.000000  \n",
       "2  0.000000  0.693147  \n",
       "3  0.000000  0.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_test2=TfidfVectorizer(Tokenizer())\n",
    "\n",
    "df=pd.DataFrame(for_test2.fit_transform(test5),columns=list(for_test2.tokenizer.word_dict.keys())[1:]) #간단하게 확인하기 위해 word_dict의 key값을 가져왔지만 실제로는 하나씩 있는걸 가져와야할 것\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bf013b729a51b2c3b026a7062c097c26d3176555f9e786b507965e3523ced33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('aibtest': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
