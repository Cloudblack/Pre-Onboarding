{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {'oov': 0}\n",
    "        self.fit_checker = False\n",
    "  \n",
    "    def preprocessing(self, sequences):       \n",
    "        # 전처리 함수 list(단수,복수 문자열), 문자열 입력가능\n",
    "        \n",
    "        def prepro(sequences):\n",
    "            token=sequences.lower()                #소문자 변경\n",
    "            token=re.sub(r\"[^a-z0-9 ]\",\"\",token)   #소문자와 공백,숫자 빼고 제거\n",
    "            token=token.split(\" \")                 #빈칸 기준으로 스플릿            \n",
    "            return token\n",
    "    \n",
    "        if type(sequences) == list:  #list인지 아닌지를 구분            \n",
    "            return [prepro(text) for text in sequences]\n",
    "        else:                        #문자열(위와 똑같이쓰면 문자별로 나눠버린다)\n",
    "            return prepro(sequences)      \n",
    "    \n",
    "  \n",
    "    def fit(self, sequences):\n",
    "        #받은 인자를 기준으로 토큰을 만든다\n",
    "        self.fit_checker = False\n",
    "        result=self.preprocessing(sequences)     \n",
    "        \n",
    "        if type(result[0]) == list:           #nested list를 flat하게 만든다\n",
    "            te=[x for y in result for x in y]\n",
    "        else:te=result\n",
    "    \n",
    "        ext_re=list(set(te))                  # 중복 제거\n",
    "        ext_re.sort()\n",
    "        \n",
    "        for x in ext_re:\n",
    "            if x not in self.word_dict:\n",
    "                self.word_dict[x]=len(self.word_dict)   # dict 갯수 = 새로 들어올 단어의 인덱스 (0이 이미 있기떄문에 -1을 안함)\n",
    "            else:pass     \n",
    "          \n",
    "        self.fit_checker = True\n",
    "        \n",
    "  \n",
    "    def transform(self, sequences):\n",
    "        #만든 토큰을 기준으로 변환을 한다\n",
    "        \n",
    "        result = []\n",
    "        tokens = self.preprocessing(sequences)\n",
    "        if self.fit_checker:\n",
    "            # output 형태를 nested list으로 맞추기위해 단수 문자열도 nested list으로 출력\n",
    "            for token in tokens:\n",
    "                token_list=[]                            \n",
    "                for word in token:\n",
    "                    if word in self.word_dict:      \n",
    "                        token_list.append(self.word_dict[word]) #있으면 변환값을 넣음\n",
    "                    else:\n",
    "                        token_list.append(self.word_dict['oov']) #없으면 oov 값 출력\n",
    "                result.append(token_list)  \n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "            \n",
    "    def fit_transform(self, sequences):\n",
    "        self.fit(sequences)\n",
    "        result = self.transform(sequences)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fit_checker = False     \n",
    "        \n",
    "    def fit(self, sequences):\n",
    "        #idf를 구하는 함수\n",
    "        tokenized = self.tokenizer.fit_transform(sequences) #tokenize로 변환된 문자열\n",
    "        tokens = list(set([x for y in tokenized for x in y])) #tokenized에서 중복을 제거한 리스트\n",
    "        tokens.sort()       \n",
    "       \n",
    "        def idf(word):\n",
    "            df=0            \n",
    "            for text in tokenized:#문서당 해당 단어를 얼마나 포함하고있는지 체크\n",
    "                df += word in text                \n",
    "            return math.log(len(tokenized)/(df+1),math.e) #log e 총 문서량/포함한 문서량 +1(분모가 0이 아니게 하려고)\n",
    "                \n",
    "        self.fit_checker = True\n",
    "        \n",
    "        return [idf(token) for token in tokens]\n",
    "      \n",
    "    \n",
    "\n",
    "    def transform(self, sequences,idf):\n",
    "        #tf-idf를 계산하는 함수\n",
    "        if self.fit_checker:\n",
    "            tokenized = self.tokenizer.transform(sequences) #tokenize로 변환된 문자열\n",
    "            tokens = list(set([x for y in tokenized for x in y])) #tokenized에서 중복을 제거한 리스트\n",
    "            tokens.sort()                            \n",
    "            def tf_idf(text):\n",
    "                #tf 문자열(문서) 마다 몇개씩 들어갔는지 * 이전에 구한 idf\n",
    "                return [text.count(tokens[n])*idf[n] for n in range(len(tokens))]\n",
    "            #문장마다 계산한다\n",
    "            return [tf_idf(text) for text in tokenized]\n",
    "        else:\n",
    "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "\n",
    "  \n",
    "    def fit_transform(self, sequences):             \n",
    "        idf=self.fit(sequences)\n",
    "        return self.transform(sequences,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #확인용 판다스\n",
    "\n",
    "test=['I go to school.', 'I LIKE pizza!']\n",
    "test2=['Im so sorry~ but i love u~ Like Pizza!']\n",
    "test3='Holly Molly 2'\n",
    "test4=['If you are not sure about your level, dont worry, you can take this online test.',\n",
    "       'There are 60 multiple-choice questions and there is no time limit.',\n",
    "       'You will be able to see the answers when you finish the test.'\n",
    "       'Improve your writing with the exercises suggested in each lesson.',\n",
    "       'You will learn how to organise and connect the text in your compositions.',\n",
    "       'Different types of texts for each level: A1, A2, B1, B1+, or B2.',]\n",
    "test5 = [\n",
    "  'eat want apple',\n",
    "  'eat want banana',\n",
    "  'long yello banana banana',\n",
    "  'i\"m furuit like'\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 8, 1], [3, 8, 2], [7, 9, 2, 2], [5, 4, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_test1=Tokenizer()\n",
    "\n",
    "for_test1.fit_transform(test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>banana</th>\n",
       "      <th>eat</th>\n",
       "      <th>furuit</th>\n",
       "      <th>im</th>\n",
       "      <th>like</th>\n",
       "      <th>long</th>\n",
       "      <th>want</th>\n",
       "      <th>yello</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      apple    banana       eat    furuit        im      like      long  \\\n",
       "0  0.693147  0.000000  0.287682  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.287682  0.287682  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.575364  0.000000  0.000000  0.000000  0.000000  0.693147   \n",
       "3  0.000000  0.000000  0.000000  0.693147  0.693147  0.693147  0.000000   \n",
       "\n",
       "       want     yello  \n",
       "0  0.287682  0.000000  \n",
       "1  0.287682  0.000000  \n",
       "2  0.000000  0.693147  \n",
       "3  0.000000  0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_test2=TfidfVectorizer(Tokenizer())\n",
    "\n",
    "df=pd.DataFrame(for_test2.fit_transform(test5),columns=list(for_test2.tokenizer.word_dict.keys())[1:]) #간단하게 확인하기 위해 word_dict의 key값을 가져왔지만 실제로는 하나씩 있는걸 가져와야할 것\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bf013b729a51b2c3b026a7062c097c26d3176555f9e786b507965e3523ced33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('aibtest': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
